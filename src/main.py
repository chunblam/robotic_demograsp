import isaacgym
import isaacgym.torch_utils as torch_utils
from isaacgym import gymapi, gymtorch, gymutil
import torch
import numpy as np
import os
import time
from config import DemoGraspConfig
from environment import DemoGraspEnvironment
from demo_recorder import DemoRecorder
from trajectory_editor import TrajectoryEditor
from single_step_mdp import SingleStepMDP
from networks import PolicyNetwork, ValueNetwork
from ppo_trainer import PPOTrainer

class DemoGraspTrainer:
    def __init__(self):
        """DemoGraspå®Œæ•´è®­ç»ƒæµç¨‹ç®¡ç†å™¨"""
        self.cfg = None
        self.env = None
        self.demo_trajectory = None
        self.mdp_env = None
        self.ppo_trainer = None
        
    def setup_training(self):
        """æ­¥éª¤1: è®­ç»ƒç¯å¢ƒè®¾ç½®"""
        print("=" * 60)
        print("æ­¥éª¤1: è®­ç»ƒç¯å¢ƒè®¾ç½®")
        print("=" * 60)
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(42)
        np.random.seed(42)
        
        # åˆ›å»ºé…ç½®å¯¹è±¡
        self.cfg = DemoGraspConfig()
        print("âœ“ é…ç½®å‚æ•°åŠ è½½å®Œæˆ")
        print(f"  æœºå™¨äºº: Kuka Allegro ({self.cfg.TOTAL_DOF} DOF)")
        print(f"  å¹¶è¡Œç¯å¢ƒ: {self.cfg.NUM_ENVS}")
        print(f"  åŠ¨ä½œç»´åº¦: {6 + self.cfg.HAND_DOF}")
        
        return self.cfg
    
    def create_simulation_environment(self):
        """æ­¥éª¤2: åˆ›å»ºä»¿çœŸç¯å¢ƒ"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤2: åˆ›å»ºä»¿çœŸç¯å¢ƒ")
        print("=" * 60)
        
        # åˆ›å»ºåŸºç¡€ä»¿çœŸç¯å¢ƒ
        self.env = DemoGraspEnvironment(self.cfg)
        print("âœ“ IsaacGymä»¿çœŸç¯å¢ƒåˆ›å»ºå®Œæˆ")
        print(f"  åŠ è½½ç‰©ä½“æ•°é‡: {len(self.env.object_assets)}")
        print(f"  æ¡Œå­å°ºå¯¸: {self.cfg.TABLE_DIMS}")
        
        return self.env
    
    def record_demonstration(self):
        """æ­¥éª¤3: å½•åˆ¶æ¼”ç¤ºè½¨è¿¹"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤3: å½•åˆ¶æ¼”ç¤ºè½¨è¿¹")
        print("=" * 60)
        
        # åˆ›å»ºæ¼”ç¤ºå½•åˆ¶å™¨
        recorder = DemoRecorder(self.env)
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ¼”ç¤ºè½¨è¿¹
        demo_path = "../output/kuka_allegro_demo.npy"
        if os.path.exists(demo_path):
            print("âœ“ åŠ è½½ç°æœ‰æ¼”ç¤ºè½¨è¿¹")
            self.demo_trajectory = recorder.load_demonstration(demo_path)
        else:
            print("âœ“ å½•åˆ¶æ–°çš„æ¼”ç¤ºè½¨è¿¹")
            self.demo_trajectory = recorder.record_demonstration(object_index=0)
            recorder.save_demonstration(self.demo_trajectory, demo_path)
        
        # æ˜¾ç¤ºæ¼”ç¤ºè½¨è¿¹ä¿¡æ¯
        print(f"  è½¨è¿¹é•¿åº¦: {len(self.demo_trajectory['hand_actions'])} æ­¥")
        print(f"  æå‡æ—¶é—´æ­¥: {self.demo_trajectory['lift_timestep']}")
        print(f"  æ‰‹éƒ¨åŠ¨ä½œç»´åº¦: {len(self.demo_trajectory['hand_actions'][0])}")
        
        return self.demo_trajectory
    
    def setup_trajectory_editor(self):
        """æ­¥éª¤4: è®¾ç½®è½¨è¿¹ç¼–è¾‘å™¨"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤4: è®¾ç½®è½¨è¿¹ç¼–è¾‘å™¨")
        print("=" * 60)
        
        # åˆ›å»ºè½¨è¿¹ç¼–è¾‘å™¨
        editor = TrajectoryEditor(self.demo_trajectory, self.cfg)
        print("âœ“ è½¨è¿¹ç¼–è¾‘å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  å¯ç¼–è¾‘å‚æ•°: SE(3)å˜æ¢ + æ‰‹éƒ¨å…³èŠ‚å¢é‡")
        print(f"  ç¼–è¾‘ç»´åº¦: {6} + {self.cfg.HAND_DOF} = {6 + self.cfg.HAND_DOF}")
        
        # æµ‹è¯•è½¨è¿¹ç¼–è¾‘åŠŸèƒ½
        T_ee, delta_qG = editor.create_random_edit_parameters()
        edited_demo = editor.edit_trajectory(T_ee, delta_qG)
        print("âœ“ è½¨è¿¹ç¼–è¾‘åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        
        return editor
    
    def create_mdp_environment(self):
        """æ­¥éª¤5: åˆ›å»ºå•æ­¥MDPç¯å¢ƒ"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤5: åˆ›å»ºå•æ­¥MDPç¯å¢ƒ")
        print("=" * 60)
        
        # åˆ›å»ºå•æ­¥MDPç¯å¢ƒ
        self.mdp_env = SingleStepMDP(self.env, self.demo_trajectory, self.cfg)
        print("âœ“ å•æ­¥MDPç¯å¢ƒåˆ›å»ºå®Œæˆ")
        
        # åˆ†æå¤æ‚åº¦å‡å°‘
        complexity_reduction = self.mdp_env.analyze_mdp_complexity()
        print(f"âœ“ æ¢ç´¢å¤æ‚åº¦å‡å°‘: {complexity_reduction:.1f}x")
        
        return self.mdp_env
    
    def setup_networks(self):
        """æ­¥éª¤6: è®¾ç½®ç¥ç»ç½‘ç»œ"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤6: è®¾ç½®ç¥ç»ç½‘ç»œ")
        print("=" * 60)
        
        # åˆ›å»ºç­–ç•¥ç½‘ç»œ
        policy_net = PolicyNetwork(self.cfg)
        value_net = ValueNetwork(self.cfg)
        
        policy_params = sum(p.numel() for p in policy_net.parameters())
        value_params = sum(p.numel() for p in value_net.parameters())
        
        print("âœ“ ç¥ç»ç½‘ç»œåˆ›å»ºå®Œæˆ")
        print(f"  ç­–ç•¥ç½‘ç»œå‚æ•°: {policy_params:,}")
        print(f"  ä»·å€¼ç½‘ç»œå‚æ•°: {value_params:,}")
        print(f"  æ€»å‚æ•°: {policy_params + value_params:,}")
        print(f"  è¾“å…¥ç»´åº¦: {self.mdp_env.state_dim}")
        print(f"  è¾“å‡ºç»´åº¦: {self.mdp_env.action_dim}")
        
        return policy_net, value_net
    
    def setup_ppo_trainer(self):
        """æ­¥éª¤7: è®¾ç½®PPOè®­ç»ƒå™¨"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤7: è®¾ç½®PPOè®­ç»ƒå™¨")
        print("=" * 60)
        
        # åˆ›å»ºPPOè®­ç»ƒå™¨
        self.ppo_trainer = PPOTrainer(self.mdp_env, self.cfg)
        print("âœ“ PPOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"  å­¦ä¹ ç‡: {self.cfg.LEARNING_RATE}")
        print(f"  PPOè£å‰ªç³»æ•°: {self.cfg.CLIP_EPS}")
        print(f"  ä»·å€¼ç³»æ•°: {self.cfg.VALUE_COEF}")
        print(f"  ç†µç³»æ•°: {self.cfg.ENTROPY_COEF}")
        
        return self.ppo_trainer
    
    def run_training(self):
        """æ­¥éª¤8: è¿è¡ŒPPOè®­ç»ƒ"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤8: è¿è¡ŒPPOè®­ç»ƒ")
        print("=" * 60)
        
        start_time = time.time()
        print("å¼€å§‹PPOè®­ç»ƒ...")
        print(f"ç›®æ ‡è¿­ä»£æ¬¡æ•°: {self.cfg.NUM_ITERATIONS}")
        
        # è¿è¡Œè®­ç»ƒ
        self.ppo_trainer.train(self.cfg.NUM_ITERATIONS)
        
        training_time = time.time() - start_time
        print(f"âœ“ è®­ç»ƒå®Œæˆ! æ€»æ—¶é—´: {training_time:.1f} ç§’")
    
    def evaluate_model(self):
        """æ­¥éª¤9: æ¨¡å‹è¯„ä¼°"""
        print("\n" + "=" * 60)
        print("æ­¥éª¤9: æ¨¡å‹è¯„ä¼°")
        print("=" * 60)
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = "best_model.pth"
        if os.path.exists(best_model_path):
            self.ppo_trainer.load_checkpoint(best_model_path)
            print("âœ“ æœ€ä½³æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_success_rate = self._evaluate_on_test_set()
            print(f"æµ‹è¯•é›†æˆåŠŸç‡: {test_success_rate:.1%}")
        else:
            print("âš  æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶")
        
        print("âœ“ è¯„ä¼°å®Œæˆ")
    
    def _evaluate_on_test_set(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        # ç®€åŒ–è¯„ä¼° - å®é™…ä¸­åº”è¯¥åœ¨æœªè§è¿‡çš„ç‰©ä½“ä¸Šæµ‹è¯•
        test_iterations = 10
        success_count = 0
        
        for i in range(test_iterations):
            state = self.mdp_env.reset()
            state_vector = self.mdp_env.get_state_vector(state)
            
            with torch.no_grad():
                action, _ = self.ppo_trainer.policy_net.sample_action(state_vector)
            
            _, reward, _, info = self.mdp_env.step(action)
            
            if info['success'][0]:
                success_count += 1
        
        return success_count / test_iterations
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¯åŠ¨ DemoGrasp å®Œæ•´è®­ç»ƒæµç¨‹")
        print("åŸºäº: 'DemoGrasp: Universal Dexterous Grasping from a Single Demonstration'")
        
        try:
            # æŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            self.setup_training()              # æ­¥éª¤1
            self.create_simulation_environment() # æ­¥éª¤2
            # self.record_demonstration()        # æ­¥éª¤3
            # self.setup_trajectory_editor()     # æ­¥éª¤4
            # self.create_mdp_environment()      # æ­¥éª¤5
            # self.setup_networks()              # æ­¥éª¤6
            # self.setup_ppo_trainer()           # æ­¥éª¤7
            # self.run_training()                # æ­¥éª¤8
            # self.evaluate_model()              # æ­¥éª¤9
            
            print("\nğŸ‰ DemoGrasp è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆ!")
            
        except Exception as e:
            print(f"\nâŒ è®­ç»ƒæµç¨‹å‡ºé”™: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    trainer = DemoGraspTrainer()
    trainer.run_complete_pipeline()

if __name__ == "__main__":
    main()